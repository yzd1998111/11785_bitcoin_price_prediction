# -*- coding: utf-8 -*-
"""IDL_Projectv0.1_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yPLFLKlahUtO9gM7THcJx50TGb1TQ5F4
"""

from datetime import datetime
now = datetime.now()
current_time = now.strftime("%Hh%Mm%Ss")
print("Run at Time =", current_time)

"""# Setup Environment
___

"""

#Set up Variables
print('Loading Environment Variables')

FP = '/content/gdrive/My Drive/IDL Final Project' 
FPD = FP

#Load Packages
print('Loading Packages')

import numpy as np
import sys
import os as os
import pickle
import pandas as pd 

import torch
import torch.nn as nn
#import torch.tensor as tensor

#from PIL import Image
#import torch.nn as nn #Import neural network framework as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from matplotlib.dates import datestr2num
from sklearn.preprocessing import MinMaxScaler
#from torchvision import datasets, transforms, models
#import matplotlib.pyplot as plt

#Load GPUs
print('Connecting to GPUs')

def check_cuda():
  _cuda = False
  if torch.cuda.is_available():
    _cuda = True
  return _cuda

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

#Start Cuda
CUDA_ON = torch.cuda.is_available()
DEVICE_STR = "cuda:0" if CUDA_ON else "cpu" #movetomanager
DEVICE = torch.device(DEVICE_STR) #movetomanager
NUM_WORKERS = 8 if CUDA_ON else 0
print('CUDA status:', check_cuda())

#Connect to Google Drive
print('Connecting to Google Drive')

#Colab connections
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

"""#Output File Handling
___

"""

#Define output dictionary handling procedures
print('Define procedures for handling output')

def save_obj(obj, name ):
    with open(name, 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)
        f.close()

def load_obj(fileloc):
    with open(fileloc, 'rb') as f:
        return pickle.load(f)
        f.close

def appendsaved_pkllist(obj, fileloc):
    filelist:list
    filelist = load_obj(fileloc)
    filelist.append(obj)
    save_obj(filelist, fileloc)

def load_run_set(RunLoad, file_dict):
    ''' 
    Load a run set with given CODE or Name RunLoad 
    '''
    found = False
    ListofDicts = load_obj(file_dict)
    for i_dict in ListofDicts:
      if (i_dict['RUNCODE'] == RunLoad) or (i_dict['RUNNAME'] == RunLoad):
        XD = i_dict
        found = True
    if found:
      print('Run set loaded:', XD['RUNNAME'], ' with code ', XD['RUNCODE'])
    else:
      print('Run not found')

#Create excel file for analysis
print('Creating output excel file for analysing results')

def CreateAnalysisFile(file_dict, file_analysis):
    XD = load_obj(file_dict) # Retrieve the dictionary
    df = pd.DataFrame(XD)
    df.to_excel(file_analysis)  
    df

"""#Data Handling
___
Define the dataset; Define supporting functions; Define Dataloaders and Load the data
"""

#Define dataset
print('Define dataset class')

class MyDataset(Dataset):
    '''
        Creates a torch dataset class
        Note modifications to __getitem__ which allow for context sizes

        Args:
          X is a Tuple of size n
          Y is a Tuple of size n i.e. both should be same size
    '''
    def __init__(self, X, Y):
        self.X = X
        self.Y = Y[:]

    def __len__(self):
        return len(self.X)

    def __getitem__(self, index):
        X = self.X[index]
        Y = self.Y[index]
        return [X, Y]

#Define load function
print('Define supporting functions ... load_XY')

def load_data(FileXY):

  data = np.loadtxt(FileXY, delimiter=',', usecols=(0,1,2), skiprows=1)
  col1 = data[:,1:2]
  scaler = MinMaxScaler(feature_range=(-1, 1))
  col1 = scaler.fit_transform(col1.reshape(-1,1))
  col2 = data[:,2:3]
  out = np.append(col1, col2, axis=1)

  return out

def expand_sparseones(tensorin, numcols=2):
    '''
    Function that takes a 1xD tensor of indices and creates a sparse onehot tensor
    Sparse matrix was used to avoid using too much RAM
    Args:
        tensorin: The input tensor - 1 dimension
        numcols: 
    Output:
        A non-sparse tensor with 1s at the indices
    Test: TODO
        Check that max index is less than numcols
        Dimensions of the input vector nx1
    '''
    #We want the matrix to be the same height as the input vector
    #(Note numcols is set globally and generally 364)
    numrows = tensorin.shape[0]
    
    tensorin = tensorin.view(1,-1)
    #tensorin.to(device)

    #Create vector of indices, ensure right dimensions with view, do it on GPU (cuda)
    #Create second vector of 1s also CUDA and combine
    Idx = torch.from_numpy(np.arange(numrows)).view(1,-1) #.cuda() 
    TensOne = torch.ones(numrows) #.cuda()
    TensLoc = torch.cat((Idx,tensorin), dim=0).type(torch.long)
    
    #Pass this to FloatTensor to build out the matrix
    tensorout = torch.sparse.FloatTensor(TensLoc, TensOne, torch.Size([numrows, numcols])).to_dense()

    return tensorout

def split_data(stock, window):

    X_data = []
    Y_data = []
        
    # create all possible sequences of length seq_len
    for index in range(len(stock) - window-1): 
        X_data.append(torch.Tensor(stock[index: index + window]))
        Y_data.append(torch.Tensor(stock[index + window: index + window+1,1]))

    test_set_size = int(np.round(0.2*len(X_data)))
    train_set_size = len(X_data) - test_set_size

    x_train = X_data[:train_set_size]
    y_train = Y_data[:train_set_size]

    x_test = X_data[train_set_size:]
    y_test = Y_data[train_set_size:]

    print(y_test)

    return x_train, y_train, x_test, y_test

"""#Define Model and all Parameters"""

class ConvBN(nn.Sequential):
    def __init__(self, in_planes, out_planes, kernel_size=5, stride=1):
        padding = (kernel_size - 1) // 2
        norm_layer = nn.BatchNorm1d
        super(ConvBN, self).__init__(
            nn.Conv1d(in_planes, out_planes, kernel_size, stride, padding, bias=False),
            norm_layer(out_planes),
            nn.ReLU6(inplace=True)
        )

class ConvLSTM(nn.Module):
    def __init__(self, input_dim, conv_dim, hidden_dim, linearhidden_dim, num_layers, output_dim, device):
        super(ConvLSTM, self).__init__()
        self.conv1 = ConvBN(input_dim, conv_dim)
        self.lstm = nn.LSTM(conv_dim, hidden_dim, num_layers, bias=False, batch_first=True)
        self.flatten = nn.Flatten(1)
        self.fc = nn.Linear(hidden_dim*num_layers, output_dim).to(device)
        self.act = nn.Sigmoid()
        self.device = device

    def forward(self, x):
        x = x.transpose(1, 2)
        x = self.conv1(x)
        x = x.transpose(1, 2)
        x, (hn, cn) = self.lstm(x) #(h0.detach(), c0.detach())
        # print("hn = ", hn.shape)
        # print("cn = ", cn.shape)
        # print("x-in = ", x.shape)
        # print('')
        x = cn.transpose(0, 1)
        x = self.flatten(x)
        x = self.fc(x)
        out = self.act(x)
        return out

# print(hP['CONV_DIM'])
# model2 = LSTM(input_dim = hP['INPUT_DIM'],
#                  conv_dim = hP['CONV_DIM'], 
#                 hidden_dim = hP['HIDDEN_DIM'],
#                 linearhidden_dim = hP['LINEARHIDDEN_DIM'], 
#                 output_dim = hP['OUTPUT_DIM'], 
#                 num_layers = hP['NUM_LAYERS'],
#                 device = DEVICE)
# _data = next(iter(train_loader))
# X, Y = _data 
# #X, Y, = X.to(DEVICE), Y.to(DEVICE)
# print("X = ", X.shape)
# output = model2(X)
# print("Output = ", output.shape)

class kpMLP(nn.Module):

  def __init__(self, input_dim, hidden_dim, linearhidden_dim, num_layers, output_dim, device):
    super(kpMLP, self).__init__()
    self.layer1 = nn.Linear(input_dim*270, 256)
    self.layer2 = nn.Linear(256, 128)
    self.layer3 = nn.Linear(128, output_dim)
    self.act = nn.Sigmoid()
    self.flatten = nn.Flatten()

  def forward(self, x):
    out = self.flatten(x)
    out = self.layer1(out)
    out = self.layer2(out)
    out = self.layer3(out)
    out = self.act(out)
    return out

class LSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, linearhidden_dim, num_layers, output_dim, device):
        super(LSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, bias=False, batch_first=True)
        self.flatten = nn.Flatten(1)
        self.fc = nn.Linear(hidden_dim*num_layers, output_dim).to(device)
        self.act = nn.Sigmoid()
        self.device = device

    def forward(self, x):
        x, (hn, cn) = self.lstm(x) #(h0.detach(), c0.detach())
        # print("hn = ", hn.shape)
        # print("cn = ", cn.shape)
        # print("x-in = ", x.shape)
        # print('')
        x = cn.transpose(0, 1)
        x = self.flatten(x)
        x = self.fc(x)
        out = self.act(x)
        return out

#Create Dictionary
RN = "Dev" #input("A name for the run: ") 

PD = dict(
    RUNNAME = RN,
    RUNCODE = "DevRun_" + current_time, #Unique run code
    DEV_MODE = True, #RunTests)
    DEBUG_MODE = True, #Enables deep debug and outputs on every iteration of training

    #RunVariables
    LOOKBACK = 270,
    INPUT_DIM = 2,
    CONV_DIM = 32,
    HIDDEN_DIM = 64,
    LINEARHIDDEN_DIM = 48,
    NUM_LAYERS = 3,
    OUTPUT_DIM = 1,

    #Optimisation Variables
    LR = 0.001,
    BATCH_SIZE = 50, #256 #Cuda Batchsize
    EPOCHS = 15,
    NUM_WORKERS = 8, 

    #Define Input files
    FOLDER_PROJECT = FP,
    FOLDER_DATA = FPD,
    #FILE_DATA = FPD + '/bitcoin_price_20120101to20191123_x.csv', #
    FILE_DATA = FPD + '/bitcoin_price_20120101to20191123_dateformat.csv', 
    FILE_SUBMIT_DATA = FPD + '/test.npy',

    #Define Output Variables
    FOLDER_OUPUT = FP + '/Output',
    FILE_RESULTS = FP + '/Output' + "/" + RN + ".csv",
    FILE_DICT = FP + "/Output"  + "/Project.pkl",
    FILE_ANALYSIS = FP + "/Output"  + "/Analysis.xlsx",

    #Define Results from the Run
    FILE_LASTRUN_MODEL = "",
    INPUT_TRAIN_SIZE = 0,
    INPUT_TEST_SIZE = 0,
    RESULTS_SAVED = False,
    RESULTS_NUM_EPOCHS = 0,
    RESULTS_TEST_LOSS = 0,
    RESULTS_CER = 0,
    RESULTS_FINAL = False #Indicator of final results
)
hP = PD

"""#Run Model"""

def Run_LoadDataSets(hP):
    print('Creating datasets ... train_dataset, test_dataset, submit_dataset')

    lookback = PD['LOOKBACK']
    data = load_data(hP['FILE_DATA'])
    x_train, y_train, x_test, y_test = split_data(data, lookback)

    train_dataset = MyDataset(x_train, y_train)
    test_dataset = MyDataset(x_test, y_test)
    submit_dataset = train_dataset #TODO: FIX THIS

    if hP['DEV_MODE']:
        print('\n', 'Load data summary')
        print("Size Train set = ", len(x_train))
        print("Size Test set = ", len(y_train))
        #print(len(x_test))
        #print(len(y_test))

    print('Creating datasets ... complete', '\n')
    
    return train_dataset, test_dataset, submit_dataset

def Run_CreateDataloaders(hP, train_dataset, test_dataset, submit_dataset):
    '''
    train_loader, test_loader, submit_loader = Run_CreateDataloaders(hP)
    '''
    print('Creating dataloaders ... ')

    train_loader_args = dict(shuffle = True, 
                              batch_size = hP['BATCH_SIZE'], 
                              num_workers = num_workers, 
                              pin_memory = True, 
                              drop_last = True) #if cuda_on else dict(shuffle=True, batch_size=64) 
    train_loader = DataLoader(train_dataset, **train_loader_args)

    test_loader_args = dict(shuffle = False, 
                            batch_size = hP['BATCH_SIZE'], 
                            num_workers = num_workers, 
                            pin_memory = True, 
                            drop_last = True)
    test_loader = DataLoader(test_dataset, **test_loader_args)

    submit_loader_args = dict(shuffle = False, 
                              batch_size = hP['BATCH_SIZE'], 
                              num_workers = num_workers, 
                              pin_memory = True, 
                              drop_last = False)
    submit_loader = DataLoader(submit_dataset, **submit_loader_args)

    if hP['DEV_MODE']:
        print('Dataloader testing summary')
        (xdata, ydata) = next(iter(test_loader))
        print('   xdata size:',xdata.shape)
        print('   ydata size:',ydata.shape)
        print(ydata[0])

    print('Creating dataloaders ... complete', '\n')

    return train_loader, test_loader, submit_loader

"""#Dev Environment Code"""

#Main training algorithm
#def main(hP : Dict, cuda_on, device, num_workers): #hP is a list of Hyper-paramters
hP = PD
cuda_on = CUDA_ON
device = DEVICE
num_workers = NUM_WORKERS

#Load datasets
train_dataset, test_dataset, submit_dataset = Run_LoadDataSets(hP)

#Create Dataloaders
train_loader, test_loader, submit_loader = Run_CreateDataloaders(hP, train_dataset, test_dataset, submit_dataset)


#Define the model
model = LSTM(input_dim = hP['INPUT_DIM'], 
                hidden_dim = hP['HIDDEN_DIM'],
                linearhidden_dim = hP['LINEARHIDDEN_DIM'], 
                output_dim = hP['OUTPUT_DIM'], 
                num_layers = hP['NUM_LAYERS'],
                device = device)
model.to(device)

#Set trainer functions
criterion = torch.nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=hP['LR'])
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader), eta_min=hP['LR'])

total_count = 0
total_trials = 0
Training_losses = []
Testing_losses = []
Training_acc = []
Testing_acc = []
TrainAcc = 0
acc = 0

#Train and test the model
print('\n', 'Begin training ...')
for epoch in range(1, hP['EPOCHS'] + 1):

    print("---EPOCH START: ", epoch)

    model.train() # Set model to training mode
    data_len = len(train_loader.dataset)                         
    print("Expected Train Cycles:", data_len)  

    for batch_idx, _data in enumerate(train_loader):
        
        optimizer.zero_grad()

        X, Y = _data 
        X = X.to(device)
        Y = Y.to(device)

        output = model(X)  # (batch, time, n_class)                             
        loss = criterion(output, Y)                                           
        loss.backward()

        Y_pred = torch.round(output)
        count = 0
        for i in range(len(Y_pred)):
          if Y_pred[i] == Y[i]:
            count += 1
        total_count = total_count + count
        total_trials = total_trials + len(Y_pred)
        TrainAcc = count/len(Y_pred)
        training_loss = loss.item()

        optimizer.step()                                                       

        if (batch_idx % 10 == 0) or (batch_idx == data_len):               
          print(batch_idx, "| Loss=", loss.item())

    model.eval()

  
    for batch_idx, _data in enumerate(test_loader):
        
        X, Y = _data                                                            #Get data and move to GPU 
        Y = Y.to(device) 
        X = X.to(device)

        output = model(X)  # (batch, time, n_class)                             #Forward pass to get output  
        loss = criterion(output, Y)                                             #Calculate loss and propogate backwards
        Y_pred = torch.round(output)

        count = 0
        for i in range(len(Y_pred)):
          if Y_pred[i] == Y[i]:
            count += 1
        Test_total_count = Test_total_count + count
        Test_total_trials = Test_total_trials + len(Y_pred)
        Acc = count/len(Y_pred)
        testing_loss = loss.item()

        #Output summary
        if (batch_idx % 10 == 0) or (batch_idx == data_len):
            print(batch_idx, "| Loss=", loss, "| Testing Acc=", Acc)

    print("----EPOCH COMPLETE:", epoch, "| Loss=", loss.item(), "Validation Acc = ", Test_total_count/Test_total_trials)
    #scheduler.step()#print("Step scheduler")#print("Learning rate =", get_lr(optimizer))

    #Save the model and results
    hP['FILE_LASTRUN_MODEL'] = hP['FOLDER_OUPUT'] + 'CurrentModel_ep' + str(epoch) + '.t7'
    print("Saving to:", hP['FILE_LASTRUN_MODEL'])
    torch.save(model.state_dict(), hP['FILE_LASTRUN_MODEL'])
    Training_losses.append(training_loss)
    Testing_losses.append(testing_loss)
    Testing_acc.append(Test_total_count/Test_total_trials)
    Training_acc.append(total_count/total_trials)
    
    total_count = 0
    total_trials = 0
    Test_total_count = 0
    Test_total_trials = 0

    hP['RESULTS_FINAL'] = False
    hP['INPUT_TRAIN_SIZE'] = len(train_dataset)
    hP['INPUT_TEST_SIZE'] = len(test_dataset)
    hP['RESULTS_SAVED'] = True
    hP['RESULTS_NUM_EPOCHS'] = epoch
    #hP['RESULTS_TEST_LOSS'] = test_loss
    #hP['RESULTS_AVGCER'] = avg_cer
    #appendsaved_pkllist(hP, hP['FILE_DICT'])

    print(""), print("")

#Save final results and create analysis file
#print('Saving final results')
#hP['RESULTS_FINAL'] = True
#appendsaved_pkllist(hP, hP['FILE_DICT'])
#CreateAnalysisFile(hP['FILE_DICT'], hP['FILE_ANALYSIS'])

from matplotlib import pyplot as plt

plt.figure()
plt.plot(range(1, len(Training_losses) + 1), new_Training_losses, label='Training losses')
plt.plot(range(1, len(Testing_losses) + 1), new_Testing_losses, label='Testing losses')
plt.xlabel('Epochs')
plt.ylabel('NLL')
plt.legend()
plt.show()



